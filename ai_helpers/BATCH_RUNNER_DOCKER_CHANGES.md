# Batch Runner Docker Integration Changes

## Summary

Modified `batch_runner.py` to use `run_docker.sh` (or docker commands directly) for PyTorch backend tests. This isolates each model run in its own container, eliminating GPU memory fragmentation issues between consecutive model runs.

## Changes Made

### 1. Modified Imports
- Added `json`, `subprocess`, and `tempfile` imports for docker execution
- Made `ai_energy_benchmarks` imports optional (only required for vLLM backend)
- Added proper error handling for missing dependencies

### 2. Added `_run_via_docker()` Method
New method that:
- Uses `run_docker.sh` to execute benchmarks in docker containers
- Uses the `text_generation.yaml` config (pre-installed in container) with hydra overrides
- Passes model-specific parameters as command-line overrides
- Parses results from `benchmark_report.json` generated by the container
- Returns results in the same format as direct execution

Key parameters passed to docker:
- `--config-name text_generation`: Uses base config from container
- `backend.model=<model_id>`: Overrides model to test
- `scenario.dataset_name=<dataset>`: Overrides dataset
- `scenario.reasoning=<bool>`: Enables/disables reasoning
- `scenario.reasoning_params.*`: Reasoning-specific parameters

### 3. Modified `run_single_model()` Method
Updated to branch based on backend type:
- **PyTorch backend**: Uses `_run_via_docker()` for isolated container execution
- **vLLM backend**: Uses direct `BenchmarkRunner` execution (existing behavior)

Benefits of docker execution:
- Each model runs in an isolated container
- GPU memory is automatically freed when container exits
- No need for complex in-process cleanup logic
- Eliminates memory fragmentation issues

### 4. Updated Cleanup Logic
Modified the `finally` block to handle both execution paths:
- **PyTorch**: Minimal cleanup (container handles everything)
- **vLLM**: Existing cleanup logic (model deletion, GPU cache clearing)

Also updated pre-run GPU cleanup to skip for PyTorch backend.

## Architecture

```
batch_runner.py (PyTorch)
    ↓
run_docker.sh
    ↓
Docker Container (energy_star image)
    ↓
entrypoint.sh
    ↓
run_ai_energy_benchmark.py
    ↓
ai_energy_benchmarks (PyTorch backend)
    ↓
Results saved to benchmark_report.json
    ↓
batch_runner.py parses results
```

## Usage

No changes to the user interface. The batch runner automatically uses docker for PyTorch backend:

```bash
# PyTorch backend (uses docker automatically)
./batch_runner.py --backend pytorch --model-name gpt-oss-20b --num-prompts 10

# vLLM backend (uses direct execution)
./batch_runner.py --backend vllm --endpoint http://localhost:8000/v1 --model-name llama-3.3-70b
```

## Testing

A test script has been created to verify the docker integration:

```bash
cd /home/scott/src/AIEnergyScore
./ai_helpers/test_batch_runner_docker.sh
```

This test:
1. Runs batch_runner with pytorch backend
2. Uses a single model with minimal prompts
3. Verifies output files are created
4. Checks for benchmark_report.json and other expected outputs

## Benefits

1. **Eliminates GPU Memory Issues**: Each model runs in isolation, no memory fragmentation
2. **Simpler Code**: No complex GPU cleanup logic needed for PyTorch backend
3. **Consistent Execution**: Same docker environment used for all PyTorch tests
4. **Better Resource Management**: Container limits and controls apply to each run
5. **Easier Debugging**: Each run is completely independent

## Environment Variables

The docker execution respects these environment variables:
- `DOCKER_IMAGE`: Docker image to use (default: "energy_star")
- `RESULTS_DIR`: Results directory (auto-set to run_dir)
- `HF_HOME`: HuggingFace cache location (default: ~/.cache/huggingface)
- `BENCHMARK_BACKEND`: Backend type (auto-set to "pytorch")

## Files Modified

- `batch_runner.py`: Main changes for docker integration
  - Imports updated
  - `_run_via_docker()` method added
  - `run_single_model()` updated for backend branching
  - Cleanup logic updated

## Files Created

- `ai_helpers/test_batch_runner_docker.sh`: Test script for docker integration
- `ai_helpers/BATCH_RUNNER_DOCKER_CHANGES.md`: This documentation

## Future Improvements

1. Add support for custom docker images per model
2. Add docker resource limits (GPU memory, CPU)
3. Add parallel container execution for multiple models
4. Add container cleanup on failure
5. Add support for docker compose for more complex setups
